<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0040)http://people.eecs.berkeley.edu/~pathak/ -->
<html class="gr__people_eecs_berkeley_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  </style>
  <link rel="icon" type="image/png" href="http://people.eecs.berkeley.edu/~pathak/images/seal_icon.png">
  <script async="" src="./images/analytics.js"></script><script type="text/javascript" src="./images/hidebib.js"></script>
  <title>Deepak Pathak</title>
  <meta name="Deepak Pathak&#39;s Berkeley Homepage" http-equiv="Content-Type" content="Deepak Pathak&#39;s Berkeley Homepage">
  <link href="./images/css" rel="stylesheet" type="text/css">
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="./images/scramble.js"></script>
</head>

<body data-gr-c-s-loaded="true">
<table width="840" border="0" align="center" cellspacing="0" cellpadding="20">
  <tbody><tr><td>

<p align="center">
    <pageheading>Norio Kosaka</pageheading><br>
    <b>email</b>:
    <font id="email" style="display:inline;">kosakaboat[at]gmail.com</font>
</p><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  

  <tbody><tr>
    <td width="32%" valign="top"><a href="./images/selfie.png"><img src="./images/selfie.png" width="100%" style="border-radius:15px"></a>
    <p align="center">
    <a href="https://drive.google.com/open?id=1fQKvBOkFSsUMuny-xQteCJN3DEpGVdgW">CV</a> | <a href="https://scholar.google.co.uk/citations?user=dIpkfPAAAAAJ&hl=en">Scholar</a> | <a href="https://github.com/Rowing0914">Github</a> | <a href="https://www.linkedin.com/in/norio-kosaka-b73701117/">LinkedIn</a>
    </p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p>
      I am a postgraduate student of MSc Machine Learning at Royal Holloway University of London and working on the intersection of Reinforcement Learning & Robotics. I am proud to be advised by <a href="http://www.cs.rhul.ac.uk/~chrisw/">Prof. Chris Watkins</a> for my Master Thesis.
    </p>

    <p>
      My ultimate goal is to build an agent which is able to play RPG games, e.g., Pokemon!! So, as you might guess, my interest lies on RL and Video Games. More specifically, I am interested in how people understand to play RPGs which require quite a comprehension to play well. Otherwise, you would end up being with a level 100 Pikachu around the bush of the first town!!
    </p>

    </td>
  </tr>
</tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
    <li> <a href="http://people.eecs.berkeley.edu/~pathak/#ASSEMBLIES19">Modular assemblies</a> won the <a href="https://virtualcreatures.github.io/" target="_blank">Virtual Creatures Competition</a> at GECCO'19!<br></li>
    <!-- <li> <a href="#ICML19">Paper</a> on real-robot exploration via disagreement accepted at ICML'19!</li> -->
    <li> Co-organized the <a href="https://futurecv.github.io/" target="_blank">"Computer Vision After 5 years"</a> workshop at CVPR'19.</li>
    <li> Co-organized the <a href="https://tarl2019.github.io/" target="_blank">Task Agnostic Reinforcement Learning</a> workshop at ICLR'19.</li>
    <li> <a href="http://people.eecs.berkeley.edu/~pathak/#ICLR19">ICLR'19 paper</a> on large-scale curiosity-driven learning covered in <a href="https://pathak22.github.io/large-scale-curiosity/index.html#media">The Economist and other media articles</a>.</li>
    <li> Co-organized the <a href="http://pocv18.eecs.berkeley.edu/">Action, Perception and Organization</a> workshop at ECCV'18.</li>
    <li> Received the <a href="https://research.fb.com/announcing-the-2018-cohort-of-facebook-fellows-and-emerging-scholars/" target="_blank">2018 Facebook Graduate Fellowship</a>. Thanks, Facebook!</li>
    <li> <a href="http://people.eecs.berkeley.edu/~pathak/#ICML18">ICML'18 paper</a> on human exploration got covered in <a href="https://rach0012.github.io/humanRL_website/index.html#media">MIT Tech Review and other media articles</a>.</li>
    <li> Received the <a href="https://snapresearchfellowship.splashthat.com/" target="_blank">2017 Snap Inc. Research Fellowship Award</a>. Thanks, Snapchat!</li>
    <li> <a href="http://people.eecs.berkeley.edu/~pathak/#ICML17">ICML'17 paper</a> on curiosity-driven exploration got covered in <a href="https://pathak22.github.io/noreward-rl/index.html#media">MIT Tech Review and other media articles</a>.</li>
    <li> Received the <a href="https://www.nvidia.com/en-us/research/graduate-fellowships/2017/deepak-pathak/" target="_blank">2017 NVIDIA Graduate Fellowship</a>. Thanks, NVIDIA! Complete list <a href="https://www.nvidia.com/en-us/research/graduate-fellowships/" target="_blank">here</a>.</li>
    <!-- <li> <a href="#NIPS17">Paper</a> accepted at NIPS 2017 on multimodal image generation.</li> -->
    <!-- <li> <a href="#ICLR18">Paper</a> on zero-shot visual imitation accepted at <a href="https://openreview.net/group?id=ICLR.cc/2018/Conference" target="_blank">ICLR 2018</a>.</li> -->
    <!-- <li> <a href="#ICML17">Paper</a> accepted at ICML 2017 on curiosity-driven exploration for reinforcement learning. <a href="http://pathak22.github.io/noreward-rl/index.html#demoVideo">Video!</a></li> -->
    <!-- <li> <a href="#CVPR17">Paper</a> accepted at CVPR 2017 on unsupervised learning using unlabeled videos.</li> -->
    <!-- <li> <a href="#CVPR16">Paper</a> accepted at CVPR 2016 on unsupervised learning and inpainting. <a href="context_encoder/">Check out!</a></li> -->
    <!-- <li> <a href="#JMLR16">Paper</a> accepted at JMLR 2016; extension of <a href="#CVPR15">CVPR'15</a> paper.</li> -->
    <!-- <li> Paper about constrained structured regression (applied to intrinsics) on <a href="http://arxiv.org/abs/1511.07497">arXiv</a>.</li> -->
    <!-- <li> <a href="#ICCV15">Paper</a> accepted at ICCV 2015 on Constrained CNN for segmentation. Code released on <a href="http://github.com/pathak22/ccnn">github</a> !</li> -->
    <!-- <li> Undergrad <a href="#JPM15">paper</a> related to predicting <a href="http://www.huffingtonpost.co.uk/2015/02/23/microsoft-oscar-predictio_n_6735122.html">Oscars</a> published at <a href="http://ubplj.org/index.php/jpm/article/view/1048">JPM</a>. See <a href="#oscarPred">live predictions</a>.</li> -->
    <!-- <li> <a href="#CVPR15">Paper</a> accepted at CVPR 2015 on domain adaptation</li> -->
    <!-- <li> <a href="#ICLR15">Paper</a> accepted at ICLR Workshop 2015</li> -->
    </ul>
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr>
    <td width="33%" valign="top" align="center"><a href="https://pathak22.github.io/exploration-by-disagreement/"><img src="./images/icml19.gif" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://pathak22.github.io/exploration-by-disagreement/" id="ICML19">
      <img src="./images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Self-Supervised Exploration via Disagreement</heading></a><br>
      <strong>Deepak Pathak*</strong>, Dhiraj Gandhi*, Abhinav Gupta<br>
      <em>International Conference on Machine Learning (ICML)</em>, 2019<br>
      </p>

      <div class="paper" id="icml19">
      <a href="https://pathak22.github.io/exploration-by-disagreement/">webpage</a> |
      <a href="https://pathak22.github.io/exploration-by-disagreement/resources/icml19.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;icml19_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;icml19&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1906.04161">arXiv</a> |
      <a href="https://pathak22.github.io/exploration-by-disagreement/index.html#sourceCode">code</a> |
      <a href="https://youtu.be/POlrWt32_ec">video</a> |
      <a href="https://videoken.com/embed/D0UmVbbJxS8?tocitem=89">oral talk</a>
      <br>

      <p align="justify"> <i id="icml19_abs" style="display: none;">Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner <i>without any external reward</i>. Notably, we further leverage the disagreement objective to optimize the agent's policy in a <i>differentiable manner</i>, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathak19disagreement,
  Author = {Pathak, Deepak and
  Gandhi, Dhiraj and Gupta, Abhinav},
  Title = {Self-Supervised Exploration
  via Disagreement},
  Booktitle = {ICML},
  Year = {2019}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://pathak22.github.io/modular-assemblies/"><img src="./images/assemblies19.gif" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://pathak22.github.io/modular-assemblies/" id="ASSEMBLIES19">
      <img src="./images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Learning to Control Self-Assembling Morphologies:<br>A Study of Generalization via Modularity</heading></a><br>
      <strong>Deepak Pathak*</strong>, Chris Lu*, Trevor Darrell, Phillip Isola and Alexei A. Efros<br>
      <em>Pre-print on arXiv:1902.05546</em>, 2019 (under review)<br>
      Winner of the <a href="https://virtualcreatures.github.io/"><em>Virtual Creatures Competition</em></a> at GECCO 2019<br>
      </p>

      <div class="paper" id="assemblies19">
      <a href="https://pathak22.github.io/modular-assemblies/">webpage</a> |
      <a href="https://pathak22.github.io/modular-assemblies/resources/assemblies.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;assemblies19_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;assemblies19&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1902.05546">arXiv</a> |
      <a href="https://youtu.be/cg-RdkPtRiQ">video</a> |
      <a href="https://pathak22.github.io/modular-assemblies/index.html#sourceCode">code</a>
      <br>

      <p align="justify"> <i id="assemblies19_abs" style="display: none;">Contemporary sensorimotor learning approaches typically start with an existing complex agent (e.g., a robotic arm), which they learn to control. In contrast, this paper investigates a modular co-evolution strategy: a collection of primitive agents learns to dynamically self-assemble into composite bodies while also learning to coordinate their behavior to control these bodies. Each primitive agent consists of a limb with a motor attached at one end. Limbs may choose to link up to form collectives. When a limb initiates a link-up action and there is another limb nearby, the latter is magnetically connected to the 'parent' limb's motor. This forms a new single agent, which may further link with other agents. In this way, complex morphologies can emerge, controlled by a policy whose architecture is in explicit correspondence with the morphology. We evaluate the performance of these <em>dynamic</em> and <em>modular</em> agents in simulated environments. We demonstrate better generalization to test-time changes both in the environment, as well as in the agent morphology, compared to static and monolithic baselines.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathak19assemblies,
  Author = {Pathak, Deepak and
  Lu, Chris and Darrell, Trevor and
  Isola, Phillip and Efros, Alexei A.},
  Title = {Learning to Control Self-
  Assembling Morphologies: A Study of
  Generalization via Modularity},
  Booktitle = {arXiv preprint arXiv:1902.05546},
  Year = {2019}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://pathak22.github.io/large-scale-curiosity/"><img src="./images/iclr19.jpg" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://pathak22.github.io/large-scale-curiosity/" id="ICLR19">
      <img src="./images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Large-Scale Study of Curiosity-Driven Learning</heading></a><br>
      Yuri Burda*, Harri Edwards*, <strong>Deepak Pathak*</strong>, Amos Storkey, Trevor Darrell and Alexei A. Efros &nbsp;&nbsp;&nbsp;&nbsp;(* equal contribution, alphabetical)<br>
      <em>International Conference on Learning Representations (ICLR)</em>, 2019<br>
      </p>

      <div class="paper" id="iclr19">
      <a href="https://pathak22.github.io/large-scale-curiosity/">webpage</a> |
      <a href="https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;iclr19_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iclr19&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1808.04355">arXiv</a> |
      <a href="https://youtu.be/l1FqtAHfJLI">video</a>
      <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
      <br>
      Also presented at NIPS'18 Deep RL Workshop (Oral)<br>

      <p align="justify"> <i id="iclr19_abs" style="display: none;">Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakICLR19largescale,
  Author = {Burda, Yuri and
  Edwards, Harri and Pathak, Deepak and
  Storkey, Amos and Darrell, Trevor and
  Efros, Alexei A.},
  Title = {Large-Scale Study of
  Curiosity-Driven Learning},
  Booktitle = {ICLR},
  Year = {2019}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://pathak22.github.io/seg-by-interaction/"><img src="./images/cvprw18.jpg" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://pathak22.github.io/seg-by-interaction/" id="CVPRW18">
      <heading>Learning Instance Segmentation by Interaction</heading></a><br>
      <strong>Deepak Pathak*</strong>, Yide Shentu*, Dian Chen*, Pulkit Agrawal*, Trevor Darrell, Sergey Levine and Jitendra Malik<br>
      <!-- <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018<br> -->
      <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br>
      </p>

      <div class="paper" id="cvprw18">
      <a href="https://pathak22.github.io/seg-by-interaction/">webpage</a> |
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/cvprw18.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;cvprw18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvprw18&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1806.08354">arXiv</a> |
      <a href="https://pathak22.github.io/seg-by-interaction/index.html#sourceCode">code</a>
      <br>

      <p align="justify"> <i id="cvprw18_abs" style="display: none;">We present an approach for building an active agent that learns to segment its visual observations into individual objects by interacting with its environment in a completely self-supervised manner. The agent uses its current segmentation model to infer pixels that constitute objects and refines the segmentation model by interacting with these pixels. The model learned from over 50K interactions generalizes to novel objects and backgrounds. To deal with noisy training signal for segmenting objects obtained by self-supervised interactions, we propose robust set loss. A dataset of robot's interactions along-with a few human labeled examples is provided as a benchmark for future research. We test the utility of the learned segmentation model by providing results on a downstream vision-based control task of rearranging multiple objects into target configurations from visual inputs alone.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakCVPRW18segByInt,
      Author = {Pathak, Deepak and
      Shentu, Yide and Chen, Dian and
      Agrawal, Pulkit and Darrell, Trevor and
      Levine, Sergey and Malik, Jitendra},
      Title = {Learning Instance Segmentation
        by Interaction},
      Booktitle = {CVPR Workshop on Benchmarks for
        Deep Learning in Robotic Vision},
      Year = {2018}
  }
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1807.07560"><img src="./images/compgan18.jpg" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/1807.07560" id="CompGAN18">
      <heading>Compositional GAN: Learning Conditional Image Composition</heading></a><br>
      Samaneh Azadi, <strong>Deepak Pathak</strong>, Sayna Ebrahimi and Trevor Darrell<br>
      Pre-print on arXiv:1807.07560, 2018
      <!-- <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018<br> -->
      </p>

      <div class="paper" id="compgan18">
      <a href="https://arxiv.org/abs/1807.07560">arXiv pdf</a> |
      <a href="javascript:toggleblock(&#39;compgan18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;compgan18&#39;)" class="togglebib">bibtex</a>
      <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
      <br>

      <p align="justify"> <i id="compgan18_abs" style="display: none;">Generative Adversarial Networks (GANs) can produce images of surprising complexity and realism, but are generally modeled to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. Capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. In this work, we propose to model object composition in a GAN framework as a self-consistent composition-decomposition network. Our model is conditioned on the object images from their marginal distributions to generate a realistic image from their joint distribution by explicitly learning the possible interactions. We evaluate our model through qualitative experiments and user evaluations in both the scenarios when either paired or unpaired examples for the individual object images and the joint scenes are given during training. Our results reveal that the learned model captures potential interactions between the two object domains given as input to output new instances of composed scene at test time in a reasonable fashion.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{azadi18compgan,
  Author = {Azadi, Samaneh and
  Pathak, Deepak and
  Ebrahimi, Sayna and Darrell, Trevor},
  Title = {Compositional GAN: Learning
  Conditional Image Composition},
  Booktitle = {arXiv:1807.07560},
  Year = {2018}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://pathak22.github.io/zeroshot-imitation/"><img src="./images/iclr18_1.gif" alt="sym" width="65%" border="1" style="border-color:black"><hr style="height:0pt; visibility:hidden; margin:0"><img src="./images/iclr18_2.gif" alt="sym" width="65%" border="1" style="border-color:black"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://pathak22.github.io/zeroshot-imitation/" id="ICLR18">
      <heading>Zero-Shot Visual Imitation</heading></a><br>
      <strong>Deepak Pathak*</strong>, Parsa Mahmoudieh*, Guanghao Luo*, Pulkit Agrawal*, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, Trevor Darrell<br>
      <em>International Conference on Learning Representations (ICLR)</em>, 2018<br>
      <!-- <strong style="color:red">Oral presentation <a href="https://vimeo.com/237270588" target="_blank" style="color:red">[video]</a></strong> -->
      <strong style="color:black">(Oral presentation)</strong>
      </p>

      <div class="paper" id="iclr18">
      <a href="http://pathak22.github.io/zeroshot-imitation/">webpage</a> |
      <!-- <a href="http://pathak22.github.io/zeroshot-imitation/resources/iclr18.pdf">pdf</a> | -->
      <a href="javascript:toggleblock(&#39;iclr18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iclr18&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1804.08606" target="_blank">arXiv</a> |
      <a href="http://pathak22.github.io/zeroshot-imitation/index.html#sourceCode">code</a> |
      <a href="http://pathak22.github.io/zeroshot-imitation/index.html#demoVideos">videos</a> |
      <a href="https://openreview.net/forum?id=BkisuzWRW" target="_blank">open-review</a> |
      <a href="https://www.dropbox.com/s/36efg1t3qn6i495/2018_04_ZeroShotImitation.pptx">slides</a>
      <br>

      <p align="justify"> <i id="iclr18_abs" style="display: none;">The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakICLR18zeroshot,
    Author = {Pathak, Deepak and
    Mahmoudieh, Parsa and Luo, Guanghao and
    Agrawal, Pulkit and Chen, Dian and
    Shentu, Yide and Shelhamer, Evan and
    Malik, Jitendra and Efros, Alexei A. and
    Darrell, Trevor},
    Title = {Zero-Shot Visual Imitation},
    Booktitle = {ICLR},
    Year = {2018}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://rach0012.github.io/humanRL_website/"><img src="./images/icml18.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://rach0012.github.io/humanRL_website/" id="ICML18">
      <heading>Investigating Human Priors for Playing Video Games</heading></a><br>
      Rachit Dubey, Pulkit Agarwal, <strong>Deepak Pathak</strong>, Thomas L. Griffiths, Alexei A. Efros<br>
      <em>International Conference on Machine Learning (ICML)</em>, 2018<br>
      <strong style="color:black">(Long oral presentation)</strong>
      </p>

      <div class="paper" id="icml18">
      <a href="https://rach0012.github.io/humanRL_website/">webpage</a> |
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/icml18.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;icml18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;icml18&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1802.10217">arXiv</a> |
      <a href="https://youtu.be/Ol0-c9OE3VQ" target="_blank">video</a>
      <br>
      Also presented at ICLR'18 Workshop track.<br>

      <p align="justify"> <i id="icml18_abs" style="display: none;">What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakICML18human,
    Author = {Dubey, Rachit and Agrawal, Pulkit
    and Pathak, Deepak and Griffiths, Thomas L.
    and Efros, Alexei A.},
    Title = {Investigating Human Priors for
    Playing Video Games},
    Booktitle = {ICML},
    Year = {2018}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://pathak22.github.io/noreward-rl/"><img src="./images/icml17_1.gif" alt="sym" width="49%" style="border-radius:15px">&nbsp;<img src="./images/icml17_2.gif" alt="sym" width="49%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://pathak22.github.io/noreward-rl/" id="ICML17">
      <heading>Curiosity-driven Exploration by Self-supervised Prediction</heading></a><br>
      <strong>Deepak Pathak</strong>, Pulkit Agrawal, Alexei A. Efros and Trevor Darrell<br>
      <em>International Conference on Machine Learning (ICML)</em>, 2017<br>
      <strong><a href="https://vimeo.com/237270588" target="_blank" style="color:black">(Oral presentation link)</a></strong>
      </p>
      <div class="paper" id="icml17">
      <a href="http://pathak22.github.io/noreward-rl/">webpage</a> |
      <a href="http://pathak22.github.io/noreward-rl/resources/icml17.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;icml17_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;icml17&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1705.05363">arXiv</a> |
      <a href="http://pathak22.github.io/noreward-rl/index.html#sourceCode">code</a> |
      <a href="http://pathak22.github.io/noreward-rl/index.html#demoVideo">video</a> |
      <a href="http://pathak22.github.io/noreward-rl/index.html#media">in the media</a>
      <br>
      Also presented at CVPR'17 <a href="http://juxi.net/workshop/deep-learning-robotic-vision-cvpr-2017/" style="color:black" target="_blank">Robotic Vision Workshop</a> (Oral)<br>

      <p align="justify"> <i id="icml17_abs" style="display: none;">In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: <i>VizDoom</i> and <i>Super Mario Bros</i>. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakICMl17curiosity,
    Author = {Pathak, Deepak and
    Agrawal, Pulkit and
    Efros, Alexei A. and
    Darrell, Trevor},
    Title = {Curiosity-driven Exploration
    by Self-supervised Prediction},
    Booktitle = {ICML},
    Year = {2017}
}</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://junyanz.github.io/BicycleGAN"><img src="./images/nips17.gif" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://junyanz.github.io/BicycleGAN" id="NIPS17">
      <heading>Toward Multimodal Image-to-Image Translation</heading></a><br>
      Jun-Yan Zhu, Richard Zhang, <strong>Deepak Pathak</strong>, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman<br>
      <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2017<br>
      </p>

      <div class="paper" id="nips17">
      <a href="https://junyanz.github.io/BicycleGAN">webpage</a> |
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/nips17.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;nips17_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;nips17&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1711.11586">arXiv</a> |
      <a href="https://github.com/junyanz/BicycleGAN">code</a> |
      <a href="https://youtu.be/JvGysD2EFhw">video</a>
      <br>

      <p align="justify"> <i id="nips17_abs" style="display: none;">Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{zhu2017multimodal,
    Author = {Zhu, Jun-Yan and Zhang, Richard
    and Pathak, Deepak and Darrell, Trevor
    and Efros, Alexei A and Wang, Oliver
    and Shechtman, Eli},
    Title = {Toward Multimodal Image-to-Image
    Translation},
    Booktitle = {NIPS},
    Year = {2017}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://people.eecs.berkeley.edu/~pathak/unsupervised_video/"><img src="./images/cvpr17.jpg" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/unsupervised_video/" id="CVPR17">
      <heading>Learning Features by Watching Objects Move</heading></a><br>
      <strong>Deepak Pathak</strong>, Ross Girshick, Piotr Dollár, Trevor Darrell and Bharath Hariharan<br>
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2017<br>
      </p>

      <div class="paper" id="cvpr17">
      <a href="http://people.eecs.berkeley.edu/~pathak/unsupervised_video/">webpage</a> |
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/cvpr17.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;cvpr17_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvpr17&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1612.06370">arXiv</a> |
      <a href="http://people.eecs.berkeley.edu/~pathak/unsupervised_video/index.html#sourceCode">code</a>
      <br>
      Also presented at CVPR'17 Workshop on <a href="https://research.google.com/youtube8m/workshop.html">YouTube-8M Large-Scale Video Understanding</a> (Oral)<br>

      <p align="justify"> <i id="cvpr17_abs" style="display: none;">This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as `pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed `pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakCVPR17learning,
    Author = {Pathak, Deepak and
    Girshick, Ross and
    Doll{\'a}r, Piotr and
    Darrell, Trevor and
    Hariharan, Bharath},
    Title = {Learning Features
    by Watching Objects Move},
    Booktitle = {CVPR},
    Year = {2017}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://people.eecs.berkeley.edu/~pathak/context_encoder/"><img src="./images/cvpr16.jpg" alt="sym" width="75%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/context_encoder/" id="CVPR16">
      <heading>Context Encoders: Feature Learning by Inpainting</heading></a><br>
      <strong>Deepak Pathak</strong>, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell and Alexei A. Efros<br>
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2016
      <br></p>

      <div class="paper" id="cvpr16">
      <a href="http://people.eecs.berkeley.edu/~pathak/context_encoder/">webpage</a> |
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/cvpr16.pdf">pdf w/ supp</a> |
      <a href="javascript:toggleblock(&#39;cvpr16_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvpr16&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1604.07379">arXiv</a> |
      <a href="https://github.com/pathak22/context-encoder">code</a> |
      <a href="http://people.eecs.berkeley.edu/~pathak/context_encoder/resources/ContextEncoder_Slides.pptx">slides</a>

      <p align="justify"> <i id="cvpr16_abs" style="display: none;">We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss.  The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakCVPR16context,
    Author = {Pathak, Deepak and
    Kr\"ahenb\"uhl, Philipp and
    Donahue, Jeff and
    Darrell, Trevor and
    Efros, Alexei A.},
    Title = {Context Encoders:
    Feature Learning by Inpainting},
    Booktitle = {CVPR},
    Year = {2016}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="./images/jmlr16.jpg"><img src="./images/jmlr16.jpg" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/papers/jmlr16.pdf" id="JMLR16">
      <heading>Large Scale Visual Recognition through Adaptation using Joint Representation and Multiple Instance Learning</heading></a><br>
      Judy Hoffman, <strong>Deepak Pathak</strong>, Eric Tzeng, Jonathan Long, Sergio Guadarrama, Trevor Darrell and Kate Saenko<br>
      <em>Journal of Machine Learning Research (JMLR)</em>, 2016
      <br></p>

      <div class="paper" id="jmlr16">
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/jmlr16.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;jmlr16_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;jmlr16&#39;)" class="togglebib">bibtex</a> |
      <a href="http://jmlr.org/papers/volume17/15-223/15-223.pdf">jmlr</a>

      <p align="justify"> <i id="jmlr16_abs" style="display: none;">A major barrier towards scaling visual recognition systems is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) trained used 1.2M+ labeled images have emerged as clear winners on object classification benchmarks. Unfortunately, only a small fraction of those labels are available with bounding box localization for training the detection task and even fewer pixel level annotations are available for semantic segmentation. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect scene-centric images with precisely localized labels. We develop methods for learning large scale recognition models which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. We provide a novel formulation of a joint multiple instance learning method that includes examples from object-centric data with image-level labels when available, and also performs domain transfer learning to improve the underlying detector representation. We then show how to use our large scale detectors to produce pixel level annotations. Using our method, we produce a &gt;7.6K category detector and release code and models at <a href="http://lsda.berkeleyvision.org/">lsda.berkeleyvision.org</a>.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakJMLR16,
    Author = {Hoffman, Judy and
    Pathak, Deepak and
    Tzeng, Eric and
    Long, Jonathan and
    Guadarrama, Sergio and
    Darrell, Trevor and
    Saenko, Kate},
    Title = {Large Scale Visual Recognition
    through Adaptation using Joint
    Representation and Multiple Instance
    Learning},
    Booktitle = {JMLR},
    Year = {2016}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="./images/iccv15.png"><img src="./images/iccv15.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/papers/iccv15.pdf" id="ICCV15">
      <heading>Constrained Convolutional Neural Networks for Weakly Supervised Segmentation</heading></a><br>
      <strong>Deepak Pathak</strong>, Philipp Krähenbühl and Trevor Darrell<br>
      <em>International Conference on Computer Vision (ICCV)</em>, 2015
      <br></p>

      <div class="paper" id="iccv15">
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/iccv15.pdf">pdf</a> |
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/iccv15_supp.pdf">supp</a> |
      <a href="javascript:toggleblock(&#39;iccv15_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iccv15&#39;)" class="togglebib">bibtex</a> |
      <a href="http://arxiv.org/abs/1506.03648">arXiv</a> |
      <a href="https://github.com/pathak22/ccnn">code</a>

      <p align="justify"> <i id="iccv15_abs" style="display: none;">We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakICCV15ccnn,
    Author = {Pathak, Deepak and
    Kr\"ahenb\"uhl, Philipp and
    Darrell, Trevor},
    Title = {Constrained Convolutional
    Neural Networks for Weakly
    Supervised Segmentation},
    Booktitle = {ICCV},
    Year = {2015}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="./images/cvpr15.png"><img src="./images/cvpr15.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/papers/cvpr15.pdf" id="CVPR15">
      <heading>Detector Discovery in the Wild: Joint Multiple Instance and Representation Learning</heading></a><br>
      Judy Hoffman, <strong>Deepak Pathak</strong>, Trevor Darrell and Kate Saenko<br>
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2015 <!-- <strong style="color:red">(Oral)</strong><br>
              <strong style="color:red">Best Student Paper Award</strong> -->
      <br></p>

      <div class="paper" id="cvpr15">
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/cvpr15.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;cvpr15_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvpr15&#39;)" class="togglebib">bibtex</a> |
      <a href="http://arxiv.org/abs/1412.1135">arXiv</a>

      <p align="justify"> <i id="cvpr15_abs" style="display: none;">We develop methods for detector learning which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category.  We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakCVPR15,
    Author = {Hoffman, Judy and
    Pathak, Deepak and
    Darrell, Trevor and
    Saenko, Kate},
    Title = {Detector Discovery
    in the Wild: Joint Multiple
    Instance and Representation
    Learning},
    Booktitle = {CVPR},
    Year = {2015}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="./images/iclr15.png"><img src="./images/iclr15.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/papers/iclr15.pdf" id="ICLR15">
      <heading>Fully Convolutional Multi-Class Multiple Instance Learning</heading></a><br>
      <strong>Deepak Pathak</strong>, Evan Shelhamer, Jonathan Long and Trevor Darrell<br>
      <em>Workshop Track in International Conf. on Learning Representations (ICLR)</em> 2015<br>
      </p>

      <div class="paper" id="iclr15">
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/iclr15.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;iclr15_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iclr15&#39;)" class="togglebib">bibtex</a> |
      <a href="http://arxiv.org/abs/1412.7144">arXiv</a>

      <p align="justify"> <i id="iclr15_abs" style="display: none;">Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakICLR15,
    Author = {Pathak, Deepak and
    Shelhamer, Evan and
    Long, Jonathan and
    Darrell, Trevor},
    Title = {Fully Convolutional
    Multi-Class Multiple Instance
    Learning},
    Booktitle = {ICLR Workshop},
    Year = {2015}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="./images/wacv15.png"><img src="./images/wacv15.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/papers/wacv15.pdf" id="WACV15">
      <heading>Anomaly Localization in Topic-based Analysis of Surveillance Videos</heading></a><br>
      <strong>Deepak Pathak</strong>, Abhijit Sharang and Amitabha Mukerjee<br>
      <em>IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 2015
      <br></p>

      <div class="paper" id="wacv15">
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/wacv15.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;wacv15_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;wacv15&#39;)" class="togglebib">bibtex</a>

      <p align="justify"> <i id="wacv15_abs" style="display: none;">Topic-models for video analysis have been used for unsupervised identification of normal activity in videos, thereby enabling the detection of anomalous actions. However, while intervals containing anomalies are detected, it has not been possible to localize the anomalous activities in such models. This is a challenging problem as the abnormal content is usually a small fraction of the entire video data and hence distinctions in terms of likelihood are unlikely. Here we propose a methodology to extend the topic based analysis with rich local descriptors incorporating quantized spatio-temporal gradient descriptors with image location and size information. The visual clips over this vocabulary are then represented in latent topic space using models like pLSA. Further, we introduce an algorithm to quantify the anomalous content in a video clip by projecting the learned topic space information. Using the algorithm, we detect whether the video clip is abnormal and if positive, localize the anomaly in spatio-temporal domain. We also contribute one real world surveillance video dataset for comprehensive evaluation of the proposed algorithm. Experiments are presented on the proposed and two other standard surveillance datasets.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakWACV15,
    Author = {Pathak, Deepak and
    Sharang, Abhijit and
    Mukerjee, Amitabha},
    Title = {Anomaly Localization
    in Topic-based Analysis of
    Surveillance Videos},
    Booktitle = {WACV},
    Year = {2015}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="./images/fg15.png"><img src="./images/fg15.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/papers/fg15.pdf" id="FG15">
      <heading>Where is my Friend? - Person identification in Social Networks</heading></a><br>
      <strong>Deepak Pathak</strong>, Sai Nitish Satyavolu and Vinay P. Namboodiri<br>
      <em>IEEE Conference on Automatic Face and Gesture Recognition (FG)</em>, 2015
      <br></p>

      <div class="paper" id="fg15">
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/fg15.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;fg15_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;fg15&#39;)" class="togglebib">bibtex</a>

      <p align="justify"> <i id="fg15_abs" style="display: none;">One of the interesting applications of computer vision is to be able to identify or detect persons in real world. This problem has been posed in the context of identifying people in television series or in multi-camera networks. However, a common scenario for this problem is to be able to identify people among images prevalent on social networks. In this paper we present a method that aims to solve this problem in real world conditions where the person can be in any pose, profile and orientation and the face itself is not always clearly visible. Moreover, we show that the problem can be solved with as weak supervision only a label whether the person is present or not, which is usually the case as people are tagged in social networks. This is challenging as there can be ambiguity in association of the right person. The problem is solved in this setting using a latent max-margin formulation where the identity of the person is the latent parameter that is classified. This framework builds on other off the shelf computer vision techniques for person detection and face detection and is able to also account for inaccuracies of these components. The idea is to model the complete person in addition to face, that too with weak supervision. We also contribute three real-world datasets that we have created for extensive evaluation of the solution. We show using these datasets that the problem can be effectively solved using the proposed method.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakFG15,
    Author = {Pathak, Deepak and
    Satyavolu, Sai Nitish and
    Namboodiri, Vinay P.},
    Title = {Where is my Friend? -
    Person identification in Social
    Networks},
    Booktitle = {Automatic Face and
    Gesture Recognition (FG)},
    Year = {2015}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><center><a href="./images/jpm15.jpg"><img src="./images/jpm15.jpg" alt="sym" width="40%" style="border-radius:15px"></a></center></td>
    <td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/papers/jpm15.pdf" id="JPM15">
      <heading>A Comparison Of Forecasting Methods: Fundamentals, Polling, Prediction Markets, and Experts</heading></a><br>
      <strong>Deepak Pathak</strong>, David Rothschild and Miro Dudík<br>
      <em>Journal of Prediction Markets (JPM)</em>, 2015
      <br></p>

      <div class="paper" id="jpm15">
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/jpm15.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;jpm15_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;jpm15&#39;)" class="togglebib">bibtex</a> |
      <a href="http://predictwise.com/blog/2014/03/oscars/">predictions2014</a> |
      <a href="http://predictwise.com/entertainment/2016-oscars" id="oscarPred">predictions2016</a>

      <p align="justify"> <i id="jpm15_abs" style="display: none;">We compare Oscar forecasts derived from four data types (fundamentals, polling, prediction markets, and domain experts) across three attributes (accuracy, timeliness and cost effectiveness). Fundamentals-based forecasts are relatively expensive to construct, an attribute the academic literature frequently ignores, and update slowly over time, constraining their accuracy. However, fundamentals provide valuable insights into the relationship between key indicators for nominated movies and their chances of victory. For instance, we find that the performance in other awards shows is highly predictive of the Oscar victory whereas box office results are not. Polling- based forecasts have the potential to be both accurate and timely. Timeliness requires incentives for frequent responses by high-information users. Accuracy is achieved by a proper transformation of raw polls. Prediction market prices are accurate forecasts, but can be further improved by simple transformations of raw prices, yielding the most accurate forecasts in our study. Expert forecasts exhibit some characteristics of fundamental models, but are generally not comparatively accurate or timely. This study is unique in both comparing and aggregating four traditional data sources, and considering critical attributes beyond accuracy. We believe that the results of this study generalize to many other domains.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakJPM15,
    Author = {Pathak, Deepak and
    Rothschild, David and
    Dudik, Miro},
    Title = {A Comparison Of Forecasting
    Methods: Fundamentals, Polling,
    Prediction Markets, and Experts},
    Booktitle = {Journal of Prediction Markets (JPM)},
    Year = {2015}
}
</pre>
      </div>
    </td>
  </tr>
</tbody></table>

<br><table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Technical Reports</sectionheading></td></tr>
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr>
    <td width="33%" valign="top"><a href="./images/iclr16.png"><img src="./images/iclr16.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://people.eecs.berkeley.edu/~pathak/papers/iclr16.pdf" id="ICLR16">
      <heading>Constrained Structured Regression with Convolutional Neural Networks</heading></a><br>
      <strong>Deepak Pathak</strong>, Philipp Krähenbühl, Stella X. Yu and Trevor Darrell<br>
      <em>arXiv:1511.07497</em>, 2015
      <br></p>

      <div class="paper" id="iclr16">
      <a href="http://people.eecs.berkeley.edu/~pathak/papers/iclr16.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;iclr16_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iclr16&#39;)" class="togglebib">bibtex</a> |
      <a href="http://arxiv.org/abs/1511.07497">arXiv</a>

      <p align="justify"> <i id="iclr16_abs" style="display: none;">Convolutional Neural Networks (CNNs) have recently emerged as the dominant model in computer vision. If provided with enough training data, they predict almost any visual quantity. In a discrete setting, such as classification, CNNs are not only able to predict a label but often predict a confidence in the form of a probability distribution over the output space. In continuous regression tasks, such a probability estimate is often lacking. We present a regression framework which models the output distribution of neural networks. This output distribution allows us to infer the most likely labeling following a set of physical or modeling constraints. These constraints capture the intricate interplay between different input and output variables, and complement the output of a CNN. However, they may not hold everywhere. Our setup further allows to learn a confidence with which a constraint holds, in the form of a distribution of the constrain satisfaction. We evaluate our approach on the problem of intrinsic image decomposition, and show that constrained structured regression significantly increases the state-of-the-art.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{pathakArxiv15,
    Author = {Pathak, Deepak and
    Kr\"ahenb\"uhl, Philipp and
    Yu, Stella X. and
    Darrell, Trevor},
    Title = {Constrained Structured
    Regression with Convolutional
    Neural Networks},
    Booktitle = {arXiv:1511.07497},
    Year = {2015}
}</pre>
      </div>
    </td>
  </tr>
</tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Teaching</sectionheading></td></tr>
</tbody></table>
<table width="100%" align="top" border="0" cellpadding="20">
  <tbody><tr>
    <td width="32%"><img src="./images/cs189.jpg" alt="pacman" width="100%" style="border-radius:15px"></td>
    <td width="68%" valign="top">
      <p>
        <a href="http://www-inst.eecs.berkeley.edu/~cs189/fa15/"><heading>CS189/289: Introduction to Machine Learning - Fall '15 (GSI) </heading></a><br>
        <strong>Instructor</strong>: Prof. Alexei A. Efros and Dr. Isabelle Guyon<br>
      </p>
      <p>
        <a href="http://www-inst.eecs.berkeley.edu/~cs280/sp16"><heading>CS280: Computer Vision - Spring '16 (GSI)</heading></a><br>
        <strong>Instructor</strong>: Prof. Trevor Darrell and Prof. Alexei A. Efros<br>
      </p>
    </td>
  </tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="10">
  <tbody><tr><td>
    <sectionheading>&nbsp;&nbsp;Selected Awards</sectionheading>
    <ul>
    <li> Facebook Graduate Fellowship (2018-2020)</li>
    <li> Nvidia Graduate Fellowship (2017-2018)</li>
    <li> Snapchat Inc. Graduate Fellowship (2017)</li>
    <li> Gold Medal in Computer Science at IIT Kanpur (2014)</li>
    <li> Best Undergraduate Thesis Award at IIT Kanpur (2014)</li>
    </ul>
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr><td><br><p align="right"><font size="2">
    Template: <a href="http://www.cs.berkeley.edu/~barron/">this</a>, <a href="http://www.cs.berkeley.edu/~sgupta/">this</a> and <a href="http://jeffdonahue.com/">this</a>
    </font></p></td></tr>
</tbody></table>

  </td></tr>
</tbody></table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jpm15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('fg15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iccv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jmlr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('nips17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvprw18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('assemblies19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('compgan18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml19_abs');
</script>



</body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>