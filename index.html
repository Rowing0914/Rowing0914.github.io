<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html class="gr__people_eecs_berkeley_edu">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
    <style type="text/css">
        a {
            color: #1772d0;
            text-decoration: none;
        }
        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }
        body, td, th {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 400
        }
        heading { font-size: 19px; font-weight: 1000 }
        strong { font-size: 16px; font-weight: 800 }
        sectionheading { font-size: 22px; font-weight: 600 }
        pageheading { font-size: 38px; font-weight: 400 }
        /* Dropdown pitch styling */
        details.pitch { margin-top: 6px; }
        details.pitch > summary {
          cursor: pointer;
          font-weight: 600;
          list-style: none;
        }
        details.pitch > summary::-webkit-details-marker { display: none; }
        details.pitch > summary::after { content: " ▼"; font-weight: 400; }
        details.pitch[open] > summary::after { content: " ▲"; }
    </style>
    <title>Norio Kosaka</title>
    <link href="./images/css" rel="stylesheet" type="text/css">
</head>

<body data-gr-c-s-loaded="true">
<table width="840" border="0" align="center" cellspacing="0" cellpadding="20">
    <tbody>
    <tr>
        <td>

            <p align="center">
                <pageheading>Norio Kosaka (小坂 紀夫)</pageheading>
                <br>
                <b>email</b>:
                <font id="email" style="display:inline;">kosakaboat[at]gmail.com</font>
            </p>

            <!-- ======= Education ======= -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                <tbody>
                <tr>
                    <td>
                        <sectionheading>&nbsp;&nbsp;Education</sectionheading>
                        <ul>
                            <li>MSc in Mathematics (Merit), <b>Birkbeck, University of London</b> <span style="float:right;">Oct. 2021 - Oct. 2023</span></li>
                            <li>Graduate Certificate in Pure Mathematics (Merit), <b>Birkbeck, University of London</b> <span style="float:right;">Jan. 2021 - Jul. 2021</span></li>
                            <li>MSc Machine Learning (Distinction), <b>Royal Holloway, University of London</b> <span style="float:right;">Sep. 2018 - Sep. 2019</span></li>
                            <li>B.A. in Commerce (GPA: 3.26), <b>Waseda University, Tokyo</b> <span style="float:right;">Apr. 2011 - Mar. 2015</span></li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- ======= Work Experience ======= -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                <tbody>
                <tr>
                    <td>
                        <sectionheading>&nbsp;&nbsp;Work Experience</sectionheading>
                        <ul>
                            <li>Researcher, <b>LY Corp.</b> <span style="float:right;">Oct. 2023 - </span></li>
                            <li>Researcher, <b>LINE Corp.</b> <span style="float:right;">Jul. 2022 - Oct. 2023</span></li>
                            <li>Research Engineer, <b>NAVER Clova AI</b> <span style="float:right;">Nov. 2020 - </span></li>
                            <li>Research Intern, <b>NAVER Clova AI</b> <span style="float:right;">Feb. 2020 - Apr. 2020</span></li>
                            <li>Research Intern, <b>Panasonic</b> <span style="float:right;">Sep. 2019 - Dec. 2019</span></li>
                            <li>Research Intern, <b>The Construct</b> <span style="float:right;">Apr. 2019 - Jul. 2019</span></li>
                            <li>DL/AI Research Engineer, <b>Rakuten Inc.</b> <span style="float:right;">Jan. 2018 - Sep. 2018</span></li>
                            <li>Data Scientist / Marketer, <b>Rakuten Inc.</b> <span style="float:right;">Apr. 2015 - Jan. 2018</span></li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- ======= Publications ======= -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                <tbody><tr><td><sectionheading>&nbsp;&nbsp;Machine Learning</sectionheading></td></tr></tbody>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>

                <!-- RLC 2025 -->
                <tr>
                  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2410.11833"><img src="./images/paper_talk/rlc2025-teaser-savo.png" width="200" height="100" style="border-radius:15px"></a></td>
                  <td width="67%" valign="top">
                    <p>
                      <a href="https://arxiv.org/abs/2410.11833"><heading>Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions</heading></a><br>
                      <strong>Ayush Jain, Norio Kosaka, Xinhu Li, Kyung-Min Kim, Erdem Bıyık, Joseph J. Lim</strong><br>
                      <em>Reinforcement Learning Conference (RLC)</em>, 2025<br>
                    </p>
                    <details class="pitch"><summary>Interview pitch</summary>
                      We introduce <b>SAVO</b>, a successive-actor method that prunes low-value regions in complex Q-landscapes so TD3 avoids local optima, delivering higher returns and better sample efficiency across MuJoCo, Adroit, RecSim and gridworld.
                    </details>
                  </td>
                </tr>

                <!-- ICML 2024 WS -->
                <tr>
                  <td width="33%" valign="top" align="center"><a href="https://openreview.net/pdf?id=9wBE2nl9Mh"><img src="./images/paper_talk/icml-2024-arlet.png" width="200" height="100" style="border-radius:15px"></a></td>
                  <td width="67%" valign="top">
                    <p>
                      <a href="https://openreview.net/pdf?id=9wBE2nl9Mh"><heading>Enhancing Actor-Critic Decision-Making with Afterstate Models</heading></a><br>
                      <strong>Norio Kosaka</strong><br>
                      <em>ICML Workshop</em>, 2024<br>
                    </p>
                    <details class="pitch"><summary>Interview pitch</summary>
                      We make critics evaluate the predicted <b>afterstate</b> (next state) rather than raw actions, simplifying value estimation; plugged into DDPG/SAC this yields faster, stabler learning on MuJoCo, PaintGym and RecSim.
                    </details>
                  </td>
                </tr>

                <!-- NeurIPS 2023 -->
                <tr>
                  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2301.12842"><img src="./images/paper_talk/NeurIPS2023_dppo.png" width="200" height="100" style="border-radius:15px"></a></td>
                  <td width="67%" valign="top">
                    <p>
                      <a href="https://arxiv.org/abs/2301.12842"><heading>Direct Preference-based Policy Optimization without Reward Modeling</heading></a><br>
                      <strong>Gaon An*, Junhyeok Lee*, Xingdong Zuo, Norio Kosaka, Kyung-Min Kim, Hyun Oh Song</strong><br>
                      <em>NeurIPS</em>, 2023<br>
                    </p>
                    <details class="pitch"><summary>Interview pitch</summary>
                      <b>DPPO</b> learns policies directly from preference labels (no reward model), improving robustness and efficiency on D4RL/Adroit/Kitchen and transferring to RLHF for LLMs.
                    </details>
                  </td>
                </tr>

                <!-- ICLR 2022 -->
                <tr>
                  <td width="33%" valign="top" align="center"><a href="https://openreview.net/forum?id=MljXVdp4A3N"><img src="./images/paper_talk/ICLR2022_AGILE_teaser.gif" width="200" height="100" style="border-radius:15px"></a></td>
                  <td width="67%" valign="top">
                    <p>
                      <a href="https://openreview.net/forum?id=MljXVdp4A3N"><heading>Know Your Action Set: Learning Action Relations for RL</heading></a><br>
                      <strong>Ayush Jain*, Norio Kosaka*, Kyung-Min Kim, Joseph J Lim</strong><br>
                      <em>ICLR</em>, 2022<br>
                    </p>
                    <details class="pitch"><summary>Interview pitch</summary>
                      <b>AGILE</b> uses a graph-attention policy to model dependencies between available actions, boosting performance and generalisation in tool-use and recommender settings.
                    </details>
                  </td>
                </tr>

                <!-- IROS 2020 -->
                <tr>
                  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/pdf/2003.00370.pdf"><img src="./images/paper_talk/IROS2020_PlaNetBayes.png" width="200" height="100" style="border-radius:15px"></a></td>
                  <td width="67%" valign="top">
                    <p>
                      <a href="https://arxiv.org/pdf/2003.00370.pdf"><heading>PlaNet of the Bayesians: Improving Deep Planning Network with Bayesian Inference</heading></a><br>
                      <strong>Masashi Okada, Norio Kosaka, Tadahiro Taniguchi</strong><br>
                      <em>IROS</em>, 2020<br>
                    </p>
                    <details class="pitch"><summary>Interview pitch</summary>
                      We add Bayesian model and action uncertainty (ensembles + probabilistic MPC) to PlaNet, improving planning robustness and sample efficiency on DeepMind Control Suite.
                    </details>
                  </td>
                </tr>

                <!-- RHUL MSc Thesis -->
                <tr>
                  <td width="33%" valign="top" align="center"><a href="https://www.researchgate.net/publication/336832476_Has_it_explored_enough"><img src="./images/paper_talk/thesis_image.png" width="200" height="100" style="border-radius:15px"></a></td>
                  <td width="67%" valign="top">
                    <p>
                      <a href="https://www.researchgate.net/publication/336832476_Has_it_explored_enough"><heading>Has it explored enough?</heading></a><br>
                      <strong>Norio Kosaka</strong><br>
                      <em>MSc Thesis, Royal Holloway</em>, 2019<br>
                    </p>
                    <details class="pitch"><summary>Interview pitch</summary>
                      An ablation-driven study of <b>DDPG</b> exploration (noise processes, BatchNorm, on/off-policy) vs <b>SAC</b>/GNN policies on MuJoCo, Centipede and gridworld, explaining when and why DDPG fails to explore.
                    </details>
                  </td>
                </tr>

                </tbody>
            </table>

        </td>
    </tr>
    </tbody>
</table>
</body>
</html>
